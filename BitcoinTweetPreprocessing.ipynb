{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f701f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"To run PySpark correctly, follow the tutorial at https://www.youtube.com/watch?v=DznteGdeJoA.\n",
    "   To avoid possible errors, the path names for the Java and Spark installations should not\n",
    "   contain any whitespace. Version of findspark used comes from pip rather than Anaconda.\n",
    "   The SparkSession is configured to run locally with 4 cores.\n",
    "   Versions of packages used:\n",
    "   Anaconda3 2021.05\n",
    "   Python 3.8.8 64-bit\n",
    "   findspark 1.4.2\n",
    "   pandas 1.2.4\n",
    "   pyspark 3.2.0\n",
    "   nltk 3.6.1\n",
    "   textblob 0.15.3\n",
    "   Spark 3.2.0\n",
    "   OpenJDK 1.8.0_41\n",
    "   re and os are built-in Python packages\n",
    "\"\"\"\n",
    "\n",
    "# https://github.com/minrk/findspark\n",
    "# https://pypi.org/project/findspark/\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "from nltk.corpus import stopwords\n",
    "from os.path import exists\n",
    "\n",
    "# Parameters for preprocessing\n",
    "# Number of rows to fetch from the input csv file\n",
    "nrows = 1000\n",
    "# Input csv filename\n",
    "data_filename = \"tweets.csv\"\n",
    "# Output csv filename\n",
    "preprocessed_data_filename = \"preprocessed_tweets.csv\"\n",
    "# Columns to select\n",
    "cols = [\"timestamp\", \"text\"]\n",
    "# Dictionary to store preprocessed information to be written to output\n",
    "timestamp_text_dict = {\"timestamp\": [], \"text\": []}\n",
    "\n",
    "# https://towardsdatascience.com/pyspark-and-sparksql-basics-6cb4bf967e53\n",
    "# https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.html\n",
    "# Using Spark for better scalability with big data and multiple machines\n",
    "# Currently cannot setup a cluster of machines to run in parallel\n",
    "# Initializing a SparkSession, important for creating DataFrames\n",
    "sc = SparkSession.builder.appName(\"BitcoinTweetPreprocessing\")\\\n",
    ".master(\"local[4]\")\\\n",
    ".config (\"spark.sql.shuffle.partitions\", \"50\")\\\n",
    ".config(\"spark.driver.maxResultSize\",\"5g\")\\\n",
    ".config (\"spark.sql.execution.arrow.enabled\", \"true\")\\\n",
    ".getOrCreate()\n",
    "\n",
    "# Check if output file already exists\n",
    "if not exists(preprocessed_data_filename):\n",
    "    # Need to use Pandas to load a small chunk of the very large csv file correctly\n",
    "    pandas_df = pd.read_csv(data_filename, sep=';', nrows=nrows, lineterminator=\"\\r\")\n",
    "    # https://sparkbyexamples.com/pyspark/convert-pandas-to-pyspark-dataframe/\n",
    "    # Converting the Pandas DataFrame into a Spark SQL DataFrame\n",
    "    spark_df = sc.createDataFrame(pandas_df)\n",
    "    # https://www.geeksforgeeks.org/how-to-loop-through-each-row-of-dataframe-in-pyspark/\n",
    "    # Select the columns and put the data in a list\n",
    "    spark_df_rows = spark_df.select(cols).collect()\n",
    "    for row in spark_df_rows:\n",
    "        # Check if the tweet text is a string and that it does not contain non-English characters\n",
    "        if type(row[1]) == str and row[1].isascii():\n",
    "            # Apply preprocessing techniques to tweet text\n",
    "            # Regular expression from https://www.geeksforgeeks.org/twitter-sentiment-analysis-using-python/\n",
    "            # Removal of hyperlinks, hashtags, usernames, and special characters\n",
    "            cleaned = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", row[1]).split())\n",
    "            # https://textblob.readthedocs.io/en/dev/quickstart.html\n",
    "            textblob = TextBlob(cleaned)\n",
    "            stop_words_removed = []\n",
    "            # Remove stop words and lemmatize the remaining words\n",
    "            # https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "            for word in textblob.words:\n",
    "                if word not in stopwords.words():\n",
    "                    lemmatized_word = Word(word).lemmatize()\n",
    "                    stop_words_removed.append(lemmatized_word)\n",
    "            # Preprocessed text and timestamp are added to the dictionary\n",
    "            timestamp_text_dict[\"text\"].append(\" \".join(stop_words_removed))\n",
    "            timestamp_text_dict[\"timestamp\"].append(row[0])\n",
    "    # Conversion of dictionary to Pandas DataFrame for writing to output csv file\n",
    "    df = pd.DataFrame.from_dict(timestamp_text_dict)\n",
    "    df.to_csv(preprocessed_data_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e039e0e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
